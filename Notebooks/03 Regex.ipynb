{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f9a4941",
   "metadata": {},
   "source": [
    "# Búsqueda con expresiones regulares\n",
    "\n",
    "Las expresiones regulares o regex son de gran utilidad para hacer búsqueda de patrones dentro de textos. Estas están asociadas a los lenguajes regulares y, por tanto, a los autómatas finitos.\n",
    "En este notebook, revisamos la implementación de un autómata que procesa una regex a partir de una adaptación del algoritmo de construcción de Thompson. \n",
    "\n",
    "### Construcción del autómata\n",
    "\n",
    "En primer lugar, determinamos un objeto que definirá al autómata. Este se define como $A = (Q, q_0, F, \\delta)$, donde $Q$ es un conjunto de estados, $q_0 \\in Q$ el estado inicial, $F$ un conjunto de estados finales y $\\delta$ la función de transición. \n",
    "\n",
    "Estos elementos definirán a la clase autómata que, además, cuenta con la <b>función de aceptación</b> de una cadena que regresa un valor boolean dependiendo si la cadena es aceptada o no por el autómata.\n",
    "También definimos funciones de <b>match</b> que señalan, dentro de un documento, si se encuentra o no el patrón buscado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "287df180",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Automata(object):\n",
    "    def __init__(self, states, initial, final, transitions):\n",
    "        \"\"\"\n",
    "        Clase para objeto Autómata finito.\n",
    "        \n",
    "        Arguments\n",
    "        ---------\n",
    "        \n",
    "        states : list\n",
    "            Lista de estados que conforman el autómata\n",
    "        initial : int\n",
    "            Estado inicial\n",
    "        final : list\n",
    "            Lista de estados finales\n",
    "        transitions : dict\n",
    "            Dictionario de transicione {q: {sym: q'}}\n",
    "        \"\"\"\n",
    "        self.states = states\n",
    "        self.initial = initial\n",
    "        self.final = final\n",
    "        self.transitions = transitions\n",
    "        \n",
    "    def __str__(self):\n",
    "        return '->'.join([str(q)+'-'+str(t) for q, t in self.transitions.items()])\n",
    "    \n",
    "    def accepts(self,string):\n",
    "        \"\"\"\n",
    "        Aplicación de autómata a cadena.\n",
    "        \n",
    "        Arguments\n",
    "        ---------\n",
    "        \n",
    "        string : str\n",
    "            Cadena de entrada para procesar por el autómata\n",
    "        \n",
    "        Returns\n",
    "        ------\n",
    "        boolean\n",
    "            True si es aceptada la cadena, False en otro caso.\n",
    "        \"\"\"\n",
    "        #Inicializa el estado\n",
    "        q = self.initial\n",
    "        for sym in string:\n",
    "            #Si no se cumple la transición, \n",
    "            #no se acepta la cadena\n",
    "            try:\n",
    "                #Transita a siguiente estado\n",
    "                #cuando esta en la función\n",
    "                q = self.transitions[q][sym]\n",
    "            except:\n",
    "                return False\n",
    "        \n",
    "        #Revisa si ha llegado a un estado final\n",
    "        return q in self.final\n",
    "    \n",
    "    def match_token(self, tokens):\n",
    "        \"\"\"\n",
    "        Encuentra el token que corresponde al patrón dentro de una documento tokenizado.\n",
    "        \n",
    "        Argumentos\n",
    "        ----------\n",
    "        tokens : list[str]\n",
    "            Lista de tokens en donde se quiere buscar el patrón.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "            Lista de índices que indican los tokens que corresponden al patrón.\n",
    "        \"\"\"\n",
    "        #Inicializa índice\n",
    "        i = 0\n",
    "        #Guarda localizaciones\n",
    "        locs = []\n",
    "        for token in tokens:\n",
    "            #Aplica la función de aceptación\n",
    "            output = self.accepts(token)\n",
    "            #Si el autómata acepta la cdena\n",
    "            if output == True:\n",
    "                #Guarda el índice\n",
    "                locs.append(i)\n",
    "            #Avanza uno en el índice\n",
    "            i += 1\n",
    "        \n",
    "        return locs\n",
    "    \n",
    "    def match(self, text):\n",
    "        \"\"\"\n",
    "        Busca el patrón del autómata dentro de un texto.\n",
    "        \n",
    "        Arguments\n",
    "        ---------\n",
    "        text : str\n",
    "            Texto (cadena) donde se buscará el patrón\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        locs : list\n",
    "            Lista de las posiciones de inicio y final del patrón.\n",
    "        \"\"\"\n",
    "        #Posición inicial en el texto\n",
    "        pos = 0\n",
    "        #Estado inicial del autómata\n",
    "        q = self.initial\n",
    "        #Guarda las posiciones\n",
    "        locs = []\n",
    "        for sym in text:\n",
    "            #Guarda la posición cuando encuentra el primer símbolo\n",
    "            first_sym = list(self.transitions[self.initial].keys())\n",
    "            if sym in first_sym:\n",
    "                #Guarda la posición cuando\n",
    "                #ha encontrada un inicio del patrón\n",
    "                begin = pos\n",
    "            #Intenta aplicar el autómata\n",
    "            try:\n",
    "                #Siguiente transición\n",
    "                q = self.transitions[q][sym]\n",
    "                #Si llega a un estado final\n",
    "                #guarda la posición donde acaba el patrón\n",
    "                if q in self.final:\n",
    "                    locs.append((begin, pos+1))\n",
    "            except:\n",
    "                #Si no encuentra el patrón\n",
    "                #continua leyendo la cadena\n",
    "                pass\n",
    "            #Avanza a la siguiente posición\n",
    "            #en el texto\n",
    "            pos += 1\n",
    "            \n",
    "        return locs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f95daf8e",
   "metadata": {},
   "source": [
    "Podemos definir un autómata muy simple y ver cuáles son las cadenas que es capaz de aceptar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f1a5485",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0-{'j': 1, 'h': 1}->1-{'a': 0, 'e': 0}\n",
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "#Función de transiciones \n",
    "#Se usa formato de diccionario\n",
    "delta = {0:{'j':1, 'h':1}, 1:{'a':0, 'e':0}}\n",
    "\n",
    "#Definimos el autómata\n",
    "fa = Automata(list(delta.keys()), 0, [0], delta)\n",
    "print(fa)\n",
    "\n",
    "#Procemos cadenas para ver si son aceptadas\n",
    "print(fa.accepts('jajaja'))\n",
    "print(fa.accepts('jaa'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b8d6756",
   "metadata": {},
   "source": [
    "La búsqueda dentro del texto puede hacerse de forma lineal, revisando todo el documento como una cadena completa. En este caso, se recorrerá caracter por caracter hasta encontrar una sucesión de caracteres que cumpla por el patrón especificado por el autómata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "93880af2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(3, 5), (9, 11), (11, 13), (13, 15)]\n",
      "ja\n",
      "ja\n",
      "je\n",
      "he\n"
     ]
    }
   ],
   "source": [
    "#Buscamos el patrón descrito por\n",
    "#el autómata dentro de un texto\n",
    "text = 'es jamón jajehe'\n",
    "matches = fa.match(text)\n",
    "\n",
    "print(matches)\n",
    "\n",
    "#Podemos localizar las coincidencias\n",
    "#dentro del texto\n",
    "for begin,end in matches:\n",
    "    print(text[begin:end])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17466480",
   "metadata": {},
   "source": [
    "Como señalamos, los documentos se componen de tokens, $d = \\{w_1, w_2, ...,w_n\\}$, por lo que otra forma de buscar dentro de los documentos será encontrando un token que coincida con el patrón definido por el autómata, o en otras palabras, encontrando un token que sea aceptado por el autómata.\n",
    "\n",
    "Para realizar esta búsqueda, definiremos una función simple de tokenización:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f72ab440",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(string):\n",
    "    \"\"\"\n",
    "    Función simple que revisa si los caracteres son alfanuméricos.\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    string : str\n",
    "        Cadena de entrada que se limpiará\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Cadena sin caracterés alfanuméricos.\n",
    "    \"\"\"\n",
    "    \n",
    "    return ''.join(c for c in string if c.isalnum())\n",
    "\n",
    "def tokenize(text):\n",
    "    \"\"\"\n",
    "    Función simple de tokenización por espacios en blanco.\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    text : documentos\n",
    "        Lista de documentos que se van a tokenizar.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    tokens \n",
    "        La lista de los documentos tokenizada.\n",
    "    \"\"\"\n",
    "    #Elimina \\n de más\n",
    "    #Pasa todo a minúscula\n",
    "    lower_text = text.strip().lower()\n",
    "    #Separa por espacio en blanco y aplica clean()\n",
    "    tokens = [clean(word) for word in lower_text.split()]\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a88d81",
   "metadata": {},
   "source": [
    "Tokenizando el texto, podemos entonces aplicar la función para buscar aquellos tokens que sean aceptados por el autómata. Obtendremos la posición de estos tokens dentro del documento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0e4a39d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['es', 'jamón', 'jajehe']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Texto tokenizado\n",
    "tokenized_text = tokenize(text)\n",
    "print(tokenized_text)\n",
    "\n",
    "#Encuentra los tokens que cumplen\n",
    "fa.match_token(tokenized_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06280c8e",
   "metadata": {},
   "source": [
    "## Algoritmo de construcción de Thompson\n",
    "\n",
    "El algoritmo de construcción de Thompson construye un autómata a partir de una cadena que representa una expresión regular. Podemos decir que este algoritmo compila la expresión regular y regresa el autómata que le corresponde (generalmente no determinístico).\n",
    "\n",
    "En este caso, hemos implenetado este algoritmo para símbolos de regex comúnes, como ?, *, +, | y concatenación. Sin embargo, puede extenderse a otros símbolos comunes en las regex. Asimismo, no implementamos el uso uso de corchetes en esta función, lo que limita las expresiones que podemos compilar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "98bec9d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile(regex):\n",
    "    \"\"\"\n",
    "    Algoritmo de Construcción de Thompson para compilar expresiones regulares.\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    regex : str\n",
    "        Expresión regular a compilar\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    Autómata finito (no determinístico) de la expresión regular.\n",
    "    \"\"\"\n",
    "    #Posición en la regex\n",
    "    pos = 0\n",
    "    #Guarda la función de transición\n",
    "    delta = {}\n",
    "    #Pila para autómatas\n",
    "    aut = []    \n",
    "    for sym in regex:\n",
    "        #Regex ?\n",
    "        if sym == '?':\n",
    "            #Observa el autómata previo\n",
    "            prev_nfa = aut.pop()\n",
    "            #Guarda la función de transición\n",
    "            delta = prev_nfa.transitions\n",
    "            #Guarda el último elemento\n",
    "            last = len(delta)-1\n",
    "            \n",
    "            #Agrega una transición epsilon a la transición previa\n",
    "            delta[last] = {**{'':last+1},**delta[last]}\n",
    "            #Crea nuevo autómata y lo gaurda en pila\n",
    "            nfa = Automata(list(delta.keys()), nfa.initial, [last,last+1], delta)\n",
    "            aut.append(nfa)\n",
    "        \n",
    "        #Regex *\n",
    "        #Símbolo previo cero o más veces\n",
    "        elif sym == '*':\n",
    "            #Observa el autómata previo\n",
    "            prev_nfa = aut.pop()\n",
    "            #Guarda la función de transición\n",
    "            delta = prev_nfa.transitions\n",
    "            #Guarda el último elemento\n",
    "            last = len(delta)-1\n",
    "            \n",
    "            #Agrega función (q,e,q') (q',e,q'') y (q',s,q')\n",
    "            delta[last] = {'':last+1}\n",
    "            delta[last+1] = {'':last+2, regex[pos-1]:last+1}\n",
    "            #Crea nuevo autómata y lo gaurda en pila\n",
    "            nfa = Automata(list(delta.keys()), nfa.initial, [last+2], delta)\n",
    "            aut.append(nfa)\n",
    "            \n",
    "        #Regex +\n",
    "        #Símbolo anterior una o más veces\n",
    "        elif sym == '+':\n",
    "            #Observa el autómata previo\n",
    "            prev_nfa = aut.pop()\n",
    "            #Guarda la función de transición\n",
    "            delta = prev_nfa.transitions\n",
    "            #Guarda el último elemento\n",
    "            last = len(delta)-1\n",
    "            \n",
    "            #Agrega reglas (q,e,q') y (q,s,q)\n",
    "            delta[last] = {'':last+1, regex[pos-1]:last}\n",
    "            #Crea nuevo autómata y lo gaurda en pila\n",
    "            nfa = Automata(list(delta.keys()), nfa.initial, [last+1], delta)\n",
    "            aut.append(nfa)\n",
    "\n",
    "        #Regex s1|s2\n",
    "        #Transita por s1 or s2\n",
    "        elif sym == '|':\n",
    "            #Observa el autómata previo\n",
    "            prev_nfa = aut.pop()\n",
    "            #Guarda la función de transición\n",
    "            delta = prev_nfa.transitions\n",
    "            #Guarda el estado más alto\n",
    "            last = max(delta.keys())\n",
    "            \n",
    "            #La transiciónes son (q,s1,q') y (q,s2,q')\n",
    "            delta[last] = {**delta[last],**{regex[pos+1]:last+1}}\n",
    "            #Crea nuevo autómata y lo gaurda en pila\n",
    "            nfa = Automata(list(delta.keys()), nfa.initial, [last+1], delta)\n",
    "            aut.append(nfa)\n",
    "            \n",
    "        #Cualquier símbolo del alfabeto\n",
    "        elif regex[pos-1] != '|':\n",
    "            #Concatenación\n",
    "            #Si hay símbolos previos\n",
    "            if aut != []:\n",
    "                #Observa el autómata previo\n",
    "                prev_nfa = aut.pop()\n",
    "                #Guarda la función de transición\n",
    "                delta = prev_nfa.transitions\n",
    "                #Guarda el estado más alto\n",
    "                last = max(delta.keys())+1\n",
    "                \n",
    "                #Agrega regla de transición (q',s,q'')\n",
    "                delta[last] = {sym:last+1}\n",
    "                #Crea nuevo autómata y lo gaurda en pila\n",
    "                nfa = Automata(list(delta.keys()), nfa.initial, [last+1], delta)\n",
    "                aut.append(nfa)\n",
    "\n",
    "            #Si es el único o primer símbolo\n",
    "            else:\n",
    "                #Estado inicial\n",
    "                initial = len(delta)\n",
    "                #Agrega regla (q,a,q')\n",
    "                delta[initial] = {sym:initial+1}\n",
    "                #Crea autómata y lo gaurda en pila\n",
    "                nfa = Automata(list(delta.keys()), 0, [len(delta)], delta)\n",
    "                aut.append(nfa)            \n",
    "        \n",
    "        #Avanza en la posición\n",
    "        #de la regex\n",
    "        pos += 1\n",
    "    \n",
    "    return aut[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c28b091",
   "metadata": {},
   "source": [
    "Podemos entonces aplicar el algoritmo a una expresión regular y observar que nos regresa un autómata que representa a esta expresión."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0d0508f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autómata 0-{'n': 1}->1-{'i': 2}->2-{'ñ': 3}->3-{'a': 4, 'o': 4}->4-{'': 5, 's': 5}\n",
      "Estados finales: [4, 5]\n"
     ]
    }
   ],
   "source": [
    "#Construye el autómata de la regex\n",
    "pattern = compile('niña|os?')\n",
    "\n",
    "print('Autómata {}\\nEstados finales: {}'.format(pattern, pattern.final))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6572972",
   "metadata": {},
   "source": [
    "También podemos ver que tipo de cadenas acepta y cuales rechaza."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c11a588b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "#Cadenas que acepta\n",
    "print(pattern.accepts('niña'))\n",
    "print(pattern.accepts('niño'))\n",
    "print(pattern.accepts('niñas'))\n",
    "print(pattern.accepts('niños'))\n",
    "print(pattern.accepts('niñs'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "287f0da1",
   "metadata": {},
   "source": [
    "Finalmente, podemos aplicar una búsqueda en un documento tokenizado para ver en dónde se encuentran los tokens que corresponden al patrón que el autómata describe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1e8650b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 niñas\n",
      "5 niño\n"
     ]
    }
   ],
   "source": [
    "#Tokenización del texto\n",
    "text = tokenize('Las niñas jugaban con el niño en el patio.')\n",
    "\n",
    "#Encuentra las correspondecias\n",
    "matches = pattern.match_token(text)\n",
    "\n",
    "#Imprime índices y tokenes\n",
    "#que cumplen el patrón dentro del texto\n",
    "for i in matches:\n",
    "    print(i, text[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cdeb8a9",
   "metadata": {},
   "source": [
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
