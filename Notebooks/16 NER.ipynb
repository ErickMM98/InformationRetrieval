{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "071c59de",
   "metadata": {},
   "source": [
    "# Etiquetado NER\n",
    "\n",
    "El etiquetado NER buscar marcar los tokens que pertenecen a una misma entidad nombrada (persona, organización, lugar, etc.) que nos ayudará a realziar recuperación de información.\n",
    "\n",
    "Implementamos un etiquetado NER simple en base al algoritmo de HMM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6084b495",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tag import hmm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from itertools import chain\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d9d6dc",
   "metadata": {},
   "source": [
    "### Preparación del dataset\n",
    "\n",
    "El dataset es un corpus con etiquetas NER y POS. Las etiquetas NER tienen un formato BIO para identificar el inicio de cadenas de entidades nombradas. Guardamos las cadenas con etiquetas NER y limpiamos el corpus. Asimismo, separamos en datos de entrenamiento y evaluación.\n",
    "\n",
    "El corpus se obtuvo de <u>https://github.com/juand-r/entity-recognition-datasets</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "00e296f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 145189/145189 [00:07<00:00, 19099.16it/s]\n"
     ]
    }
   ],
   "source": [
    "#Abre el documento con corpus\n",
    "text = open('NERSpanish.txt','r').read().strip().split('\\n')\n",
    "\n",
    "#Corpus NER\n",
    "corpus_NER = []\n",
    "#Corpus NER+POS\n",
    "corpus_FULL = []\n",
    "for sent in tqdm(text):\n",
    "    #Guarda oraciones con POS+NER\n",
    "    tagged_sent = []\n",
    "    #Guarda oraciones con NER\n",
    "    NER_sent = []\n",
    "    for pack in sent.split():\n",
    "        #Separa en token, POS, NER\n",
    "        w, pos, ner = pack.split('|')\n",
    "        #Guarda POS+NER\n",
    "        tagged_sent.append((w,pos,ner))\n",
    "        #Guarda NER\n",
    "        NER_sent.append((w,ner))\n",
    "    corpus_FULL.append(tagged_sent)\n",
    "    corpus_NER.append(NER_sent)\n",
    "\n",
    "#Elimina elementos vacíos del corpus NER\n",
    "corpus = [sent for sent in corpus_NER if sent != []]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8714651",
   "metadata": {},
   "source": [
    "Finalmente, obtenemos los datos de entrenamiento y evaluación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf96f87c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "115519 12836\n"
     ]
    }
   ],
   "source": [
    "#Dividimos en evaluación y entrenamiento\n",
    "train_data, test_data = train_test_split(corpus, test_size=0.1)\n",
    "\n",
    "print(len(train_data), len(test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62eed333",
   "metadata": {},
   "source": [
    "## Modelo oculto de Markov\n",
    "\n",
    "### Entrenamiento\n",
    "\n",
    "Tomamos los datasets de entrenamiento y generamos en modelo en base a la biblioteca de NLTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e49bd97",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Genera el modelo\n",
    "HMM = hmm.HiddenMarkovModelTrainer()\n",
    "#Entrena el modelo\n",
    "NERtagger = HMM.train_supervised(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "356fdb5d",
   "metadata": {},
   "source": [
    "### Evaluación\n",
    "\n",
    "Evaluamos el modelo con base en el dataset de evaluación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dbc01128",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12836/12836 [03:01<00:00, 70.56it/s] \n"
     ]
    }
   ],
   "source": [
    "#Etiquetas reales\n",
    "Y = []\n",
    "#Etiquetas predichas\n",
    "Y_pred = []\n",
    "for test_sent in tqdm(test_data):\n",
    "    #Observaciones y emisiones\n",
    "    x,y = zip(*test_sent)\n",
    "    #Predicción\n",
    "    y_pred = NERtagger.tag(list(x))\n",
    "    \n",
    "    #Guarda las etiquetas\n",
    "    Y.append(list(y))\n",
    "    Y_pred.append([tag[1] for tag in y_pred])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "664ed8c0",
   "metadata": {},
   "source": [
    "Obtenemos el reporte de clasificación para cada etiqueta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1b5f6297",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mijangos/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/mijangos/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       B-LOC       0.00      0.00      0.00         6\n",
      "      B-MISC       0.00      0.00      0.00         2\n",
      "       B-ORG       0.00      0.00      0.00         0\n",
      "       B-PER       0.00      0.00      0.00         6\n",
      "       I-LOC       0.92      0.73      0.81     18080\n",
      "      I-MISC       0.84      0.57      0.68      6932\n",
      "       I-ORG       0.88      0.70      0.78      4015\n",
      "       I-PER       0.92      0.75      0.82     11887\n",
      "           O       0.97      1.00      0.98    309535\n",
      "\n",
      "    accuracy                           0.96    350463\n",
      "   macro avg       0.50      0.42      0.45    350463\n",
      "weighted avg       0.96      0.96      0.96    350463\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(list(chain(*Y)), list(chain(*Y_pred))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "976cf1bb",
   "metadata": {},
   "source": [
    "Podemos probar cómo funciona en un caso específico:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9b5f8402",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Yo', 'O'),\n",
       " ('vivo', 'O'),\n",
       " ('en', 'O'),\n",
       " ('el', 'O'),\n",
       " ('Valle', 'I-LOC'),\n",
       " ('de', 'I-LOC'),\n",
       " ('México', 'I-LOC')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent = 'Yo vivo en el Valle de México'\n",
    "NERtagger.tag(sent.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd6077c5",
   "metadata": {},
   "source": [
    "     "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
